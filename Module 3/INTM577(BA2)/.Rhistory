library(twitteR)
library(ggplot2)
library(syuzhet)
library(tm)
appname <- "DipK"
key <- "hVmkA8klKaRoyaWIMfcKVI8In"
secret <- "nJ8CQLyG79NSS6kjqdi3A3sJM0DiZkxkMLkj7DiVtwQH9V01fc"
access<-"1089743576982945792-k0QnskV7AHxE7becjFFBngIWoaKsJU"
access_secret="fWCs6agIte2x6H7lYq5L1UnjqQDQ2Zqqg37FBla6E3H1s"
setup_twitter_oauth(key, secret, access, access_secret)
tweets_tech <- searchTwitter("BUDGET 2023", n=100,lang = "en")
a <- twListToDF(tweets_tech)
corpus = iconv(a$text, "latin1", "UTF-8")
corpus<- Corpus(VectorSource(corpus))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs=corpus
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
corpus=docs
corpus<- tm_map(corpus,tolower)
corpus<-tm_map(corpus,removePunctuation)# remove puntuations like , .
corpus<- tm_map(corpus,removeNumbers)
cleanset<-tm_map(corpus,removeWords,stopwords('english'))# remove common words
removeURL<- function(x)gsub('http[[:alnum:]]=','',x)
cleanset<-tm_map(cleanset,content_transformer(removeURL))
x=cleanset
tdm4<-TermDocumentMatrix(cleanset)
tdm4 # display information
tdm4<-as.matrix(tdm4)
#=====================================
v=sort(rowSums(tdm4))
library(wordcloud)
w<-data.frame(names(v),v)
colnames(w)<-c('word','freq')
set.seed(1234)
wordcloud(words=w$word,freq=w$freq)
library(wordcloud2)
letterCloud(w,
word="R",
size=5,
color="rainbow")
letterCloud(w, word = "WORDCLOUD2", wordSize = 1)
wordcloud2(w, size=10,color = "random-light", backgroundColor = "grey")
wordcloud2(w,size=5,shape = 'circle')
write.csv(tdm4,"tdm4.csv")
library(syuzhet)
data=read.csv("tdm4.csv")
mysentiment_tech<-get_nrc_sentiment((data$X))
Sentimentscores_tech<-data.frame(colSums(mysentiment_tech[,]))
names(Sentimentscores_tech)<-"Score"
Sentimentscores_tech<-cbind("sentiment"=rownames(Sentimentscores_tech),Sentimentscores_tech)
rownames(Sentimentscores_tech)<-NULL
ggplot(data=Sentimentscores_tech,aes(x=sentiment,y=Score))+
geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Budget 2023")
a=read.csv(file.choose(),header=TRUE)
corpus = iconv(a$text, "latin1", "UTF-8")
corpus<- Corpus(VectorSource(corpus))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs=corpus
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
corpus=docs
corpus<- tm_map(corpus,tolower)
library(twitteR)
library(ggplot2)
library(syuzhet)
library(tm)
appname <- "DipK"
key <- "hVmkA8klKaRoyaWIMfcKVI8In"
secret <- "nJ8CQLyG79NSS6kjqdi3A3sJM0DiZkxkMLkj7DiVtwQH9V01fc"
a
x
tdm4
v
data
w
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
#
View(Sentimentscores_tech)
library(twitteR)
library(ggplot2)
library(syuzhet)
library(tm)
appname <- "DipK"
key <- "hVmkA8klKaRoyaWIMfcKVI8In"
secret <- "nJ8CQLyG79NSS6kjqdi3A3sJM0DiZkxkMLkj7DiVtwQH9V01fc"
access<-"1089743576982945792-k0QnskV7AHxE7becjFFBngIWoaKsJU"
access_secret="fWCs6agIte2x6H7lYq5L1UnjqQDQ2Zqqg37FBla6E3H1s"
setup_twitter_oauth(key, secret, access, access_secret)
tweets_tech <- searchTwitter("#Budget2023", n=500,lang = "en")
a <- twListToDF(tweets_tech)
# library(tm)
corpus = iconv(a$text, "latin1", "UTF-8")
corpus<- Corpus(VectorSource(corpus))
# corpus==>Documents/Docs
# VectorSource==>vector
# a$text==> row/records
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs=corpus
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
corpus=docs
corpus<- tm_map(corpus,tolower)
corpus<-tm_map(corpus,removePunctuation)# remove puntuations like , .
corpus<- tm_map(corpus,removeNumbers)
cleanset<-tm_map(corpus,removeWords,stopwords('english'))# remove common words
removeURL<- function(x)gsub('http[[:alnum:]]=','',x)
cleanset<-tm_map(cleanset,content_transformer(removeURL))
x=cleanset
tdm4<-TermDocumentMatrix(cleanset)
tdm4 # display information
tdm4<-as.matrix(tdm4)
#=====================================
v=sort(rowSums(tdm4))
library(wordcloud)
w<-data.frame(names(v),v)
colnames(w)<-c('word','freq')
set.seed(1234)
wordcloud(words=w$word,freq=w$freq)
library(wordcloud2)
letterCloud(w,
word="R",
size=5,
color="rainbow")
letterCloud(w, word = "WORDCLOUD2", wordSize = 1)
wordcloud2(w, size=10,color = "random-light", backgroundColor = "grey")
wordcloud2(w,size=5,shape = 'circle')
#===================================
write.csv(tdm4,"tdm4.csv")
library(syuzhet)
data=read.csv("tdm4.csv")
mysentiment_tech<-get_nrc_sentiment((data$X))
#calculationg total score for each sentiment
Sentimentscores_tech<-data.frame(colSums(mysentiment_tech[,]))
names(Sentimentscores_tech)<-"Score"
Sentimentscores_tech<-cbind("sentiment"=rownames(Sentimentscores_tech),Sentimentscores_tech)
rownames(Sentimentscores_tech)<-NULL
#*************************************************************************************
ggplot(data=Sentimentscores_tech,aes(x=sentiment,y=Score))+
geom_bar(aes(fill=sentiment),stat = "identity")+
theme(legend.position="none")+
xlab("Sentiments")+ylab("scores")+ggtitle("Budget 2023")
library(tuber) # youtube API
library(magrittr) # Pipes %>%, %T>% and equals(), extract().
library(tidyverse) # all tidyverse packages
library(purrr) # package for iterating/extracting data
client_id <- "375771870105-dlhu23ucu0r42qc029v0a9jhkjabaqml.apps.googleusercontent.com"
client_secret <- "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"
# use the youtube oauth
yt_oauth(app_id = client_id,
app_secret = client_secret,token= '')
# use the youtube oauth
yt_oauth(app_id = client_id,
app_secret = client_secret,token= '')
install.packages("tuber")
install.packages("tidyverse")
library(tuber) # youtube API
library(magrittr) # Pipes %>%, %T>% and equals(), extract().
library(tidyverse) # all tidyverse packages
library(purrr) # package for iterating/extracting data
client_id <- "375771870105-dlhu23ucu0r42qc029v0a9jhkjabaqml.apps.googleusercontent.com"
client_secret <- "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"
# use the youtube oauth
yt_oauth(app_id = client_id,
app_secret = client_secret,token= '')
install.packages("RGoogleAnalytics")
library(RGoogleAnalyticsPremium)
library(RGoogleAnalyticsPremium)
client_id <- "375771870105-dlhu23ucu0r42qc029v0a9jhkjabaqml.apps.googleusercontent.com"
client_secret <- "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"
token<- Auth(client_id,client_secret)
ValidateToken(token)
token<- Auth(client_id,client_secret)
client_secret <- "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"
token<- Auth(client_id,client_secret)
ValidateToken(token)
query.list <- Init(start.date="2020-09-22",
end.date="2020-09-23",
dimensions="ga:month,ga.year",
metrics="ga:itemRevenue",
max.results = 100,
sort="ga:year",
table.id="ga:92320289")
ga.query<-QueryBuilder(query.list)
query.list <- Init(start.date="2020-09-22",
end.date="2020-09-23",
dimensions="ga:month,ga.year",
metrics="ga:itemRevenue",
max.results = 100,
sort="ga:year",
table.id="ga:92320289")
query.list <- Init(start.date="2023-03-03",
end.date="2023-03-03",
dimensions="ga:month,ga.year",
metrics="ga:itemRevenue",
max.results = 100,
sort="ga:year",
table.id="ga:92320289")
install.packages("RGoogleAnalytics")
library(RGoogleAnalyticsPremium)
client_id <- "375771870105-dlhu23ucu0r42qc029v0a9jhkjabaqml.apps.googleusercontent.com"
client_secret <- "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"
token<- Auth(client_id,client_secret)
ValidateToken(token)
query.list <- Init(start.date="2023-03-03",
end.date="2023-03-03",
dimensions="ga:month,ga.year",
metrics="ga:itemRevenue",
max.results = 100,
sort="ga:year",
table.id="ga:92320289")
query.list <- Init(start.date="2023-03-03",
end.date="2023-03-03",
dimensions="ga:month,ga.year",
metrics="ga:itemRevenue")
ga.query<-QueryBuilder(query.list)
ga.data<-GetReportData(ga.query,token)
head(ga.data)
ga.data<-GetFile(ga.query,token)
head(ga.data)
library(rvest)
library(stringr)
url <- 'http://books.toscrape.com/catalogue/category/books/travel_2/index.html'
titles <- read_html(url) %>%
html_nodes('h3') %>%
html_nodes('a') %>%
html_text()
urls <- read_html(url) %>%
html_nodes('.image_container') %>%
html_nodes('a') %>%
html_attr('href') %>%
str_replace_all('../../../', '/')
imgs <- read_html(url) %>%
html_nodes('.image_container') %>%
html_nodes('img') %>%
html_attr('src') %>%
str_replace_all('../../../../', '/')
ratings <- read_html(url) %>%
html_nodes('p.star-rating') %>%
html_attr('class') %>%
str_replace_all('star-rating ', '')
prices <- read_html(url) %>%
html_nodes('.product_price') %>%
html_nodes('.price_color') %>%
html_text()
availability <- read_html(url) %>%
html_nodes('.product_price') %>%
html_nodes('.instock') %>%
html_text() %>%
str_trim()
scraped <- data.frame(
Title = title,
URL = urls,
SourceImage = imgs,
Rating = ratings,
Price = prices,
Availability = availability
)
scraped
#Specifying the url for desired website to be scrapped
url <-  read_html( "https://www.amazon.in/OnePlus-Mirror-Black-64GB-Memory/dp/B0756Z43QS?tag=googinhydr18418-21&tag=googinkenshoo-21&ascsubtag=aee9a916-6acd-4409-92ca-3bdbeb549f80")
#scrape title of the product> title_html <- html_nodes(webpage, 'h1#title')>
title <- html_text(url)
head(title)
title
size <- url %>%
html_nodes(".a-size-small") %>%
html_text()
size
review <- url %>%
html_nodes(".a-size-base") %>%
html_text()
review
head(review)
tail(review)
t <- url %>%
html_nodes("table") %>%
.[[1]]%>%
html_table()
t
url %>%
html_nodes("p") %>%
html_text()
url
url %>% html_node("div h1")%>% html_text()
url
library(rvest)
library(stringr)
url <- 'http://books.toscrape.com/catalogue/category/books/travel_2/index.html'
titles <- read_html(url) %>%
html_nodes('h3') %>%
html_nodes('a') %>%
html_text()
urls <- read_html(url) %>%
html_nodes('.image_container') %>%
html_nodes('a') %>%
html_attr('href') %>%
str_replace_all('../../../', '/')
imgs <- read_html(url) %>%
html_nodes('.image_container') %>%
html_nodes('img') %>%
html_attr('src') %>%
str_replace_all('../../../../', '/')
ratings <- read_html(url) %>%
html_nodes('p.star-rating') %>%
html_attr('class') %>%
str_replace_all('star-rating ', '')
prices <- read_html(url) %>%
html_nodes('.product_price') %>%
html_nodes('.price_color') %>%
html_text()
availability <- read_html(url) %>%
html_nodes('.product_price') %>%
html_nodes('.instock') %>%
html_text() %>%
str_trim()
scraped <- data.frame(
Title = title,
URL = urls,
SourceImage = imgs,
Rating = ratings,
Price = prices,
Availability = availability
)
scraped
availability
View(scraped)
View(scraped)
#Specifying the url for desired website to be scrapped
url <-  read_html( "https://www.amazon.in/OnePlus-Mirror-Black-64GB-Memory/dp/B0756Z43QS?tag=googinhydr18418-21&tag=googinkenshoo-21&ascsubtag=aee9a916-6acd-4409-92ca-3bdbeb549f80")
#scrape title of the product> title_html <- html_nodes(webpage, 'h1#title')>
title <- html_text(url)
head(title)
title
size <- url %>%
html_nodes(".a-size-small") %>%
html_text()
size
review <- url %>%
html_nodes(".a-size-base") %>%
html_text()
review
head(review)
tail(review)
t <- url %>%
html_nodes("table") %>%
.[[1]]%>%
html_table()
t
url %>%
html_nodes("p") %>%
html_text()
url
url %>% html_node("div h1")%>% html_text()
View(url)
t
